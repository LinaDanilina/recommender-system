{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wide and Deep Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMmPHkG9QSWpht/3TnvGjo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LinaDanilina/recommender-system/blob/master/Wide_and_Deep_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45kkXFcv28lU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3f6c4fd0-22ad-4a5c-9055-1292a411e87c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViIzsSar3D8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e96ea14-8d56-4548-f4f0-71101309a545"
      },
      "source": [
        "%cd drive/My Drive/recommenders"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/recommenders\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Q9AKDzX_BV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "e58e11b4-6dce-4d2b-d038-c8f707abeb16"
      },
      "source": [
        "!pip install scrapbook"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scrapbook\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/22/5c55a90934780daedc2747c8e40bae814adff393da1aca0fae577d13c00d/scrapbook-0.2.0.tar.gz\n",
            "Collecting parsel>=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scrapbook) (1.12.0)\n",
            "Collecting w3lib>=1.19.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
            "Collecting cssselect>=0.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from parsel>=1.2.0->scrapbook) (4.2.6)\n",
            "Building wheels for collected packages: scrapbook\n",
            "  Building wheel for scrapbook (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scrapbook: filename=scrapbook-0.2.0-cp36-none-any.whl size=4883 sha256=868278dd8aee8231cd7c533f05b34818f081241b4f490dd1ba6510e594a49b00\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/4e/3e/b23f6c97e73fdcae44d7a5c8c22024b8e9410c1a20fe61f864\n",
            "Successfully built scrapbook\n",
            "Installing collected packages: w3lib, cssselect, parsel, scrapbook\n",
            "Successfully installed cssselect-1.1.0 parsel-1.5.2 scrapbook-0.2.0 w3lib-1.21.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdXI43Az3js0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "3388134a-70d4-4c4b-b19f-726ac8e225e4"
      },
      "source": [
        "!pip install papermill"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting papermill\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/83/4063c0622a465f677736b4fcbbdbf50d18fcd78f4ae7211f1e17591e1ed2/papermill-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from papermill) (7.1.1)\n",
            "Collecting tenacity\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/30/4d1205370f249f6e3ff0d29f8eb3aebfb7f88bf925364a222c6abcbc383e/tenacity-6.1.0-py2.py3-none-any.whl\n",
            "Collecting black; python_version >= \"3.6\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/bb/ad34bbc93d1bea3de086d7c59e528d4a503ac8fe318bd1fa48605584c3d2/black-19.10b0-py36-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.6/dist-packages (from papermill) (4.38.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from papermill) (3.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from papermill) (5.3.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from papermill) (2.21.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from papermill) (0.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from papermill) (5.0.4)\n",
            "Collecting ansiwrap\n",
            "  Downloading https://files.pythonhosted.org/packages/03/50/43e775a63e0d632d9be3b3fa1c9b2cbaf3b7870d203655710a3426f47c26/ansiwrap-0.8.4-py2.py3-none-any.whl\n",
            "Collecting nbclient\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/61/b9bfa39e6d9688607c559ba8527fff950bd06a7ed840787565f04194333f/nbclient-0.1.0-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from tenacity->papermill) (1.12.0)\n",
            "Collecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from black; python_version >= \"3.6\"->papermill) (19.3.0)\n",
            "Collecting toml>=0.9.4\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/12/ced7105d2de62fa7c8fb5fce92cc4ce66b57c95fb875e9318dba7f8c5db0/toml-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from black; python_version >= \"3.6\"->papermill) (2019.12.20)\n",
            "Collecting typed-ast>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 7.8MB/s \n",
            "\u001b[?25hCollecting pathspec<1,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/34/fa/c5cc4f796eb954b56fd1f6c7c315647b18b027e0736c9ae87b73bbb1f933/pathspec-0.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from jupyter-client->papermill) (4.3.3)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->papermill) (4.5.3)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->papermill) (4.6.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->papermill) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->papermill) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->papermill) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->papermill) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->papermill) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->papermill) (3.0.4)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->papermill) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->papermill) (0.2.0)\n",
            "Collecting textwrap3>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/77/9c/a53e561d496ee5866bbeea4d3a850b3b545ed854f8a21007c1e0d872e94d/textwrap3-0.9.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->jupyter-client->papermill) (4.4.2)\n",
            "Installing collected packages: tenacity, appdirs, toml, typed-ast, pathspec, black, textwrap3, ansiwrap, nbclient, papermill\n",
            "Successfully installed ansiwrap-0.8.4 appdirs-1.4.3 black-19.10b0 nbclient-0.1.0 papermill-2.0.0 pathspec-0.7.0 tenacity-6.1.0 textwrap3-0.9.2 toml-0.10.0 typed-ast-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0B_mQso3Ni7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "3b2c11d6-dfc2-413f-aabf-1499f59bb808"
      },
      "source": [
        "import itertools\n",
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "import numpy as np\n",
        "import papermill as pm\n",
        "import pandas as pd\n",
        "import sklearn.preprocessing\n",
        "import tensorflow as tf\n",
        "\n",
        "from reco_utils.common.constants import (\n",
        "    DEFAULT_USER_COL as USER_COL,\n",
        "    DEFAULT_ITEM_COL as ITEM_COL,\n",
        "    DEFAULT_RATING_COL as RATING_COL,\n",
        "    DEFAULT_PREDICTION_COL as PREDICT_COL,\n",
        "    SEED\n",
        ")\n",
        "from reco_utils.common import tf_utils, gpu_utils, plot\n",
        "from reco_utils.dataset import movielens\n",
        "from reco_utils.dataset.pandas_df_utils import user_item_pairs\n",
        "from reco_utils.dataset.python_splitters import python_random_split\n",
        "import reco_utils.evaluation.python_evaluation as evaluator\n",
        "import reco_utils.recommender.wide_deep.wide_deep_utils as wide_deep\n",
        "\n",
        "print(\"Tensorflow Version:\", tf.VERSION)\n",
        "print(\"GPUs:\\n\", gpu_utils.get_gpu_info())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:13: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:14: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:15: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:16: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:17: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:18: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:19: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:242: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
            "\n",
            "Tensorflow Version: 1.15.0\n",
            "GPUs:\n",
            " [{'device_name': 'Tesla K80', 'total_memory': 11441.1875, 'free_memory': 11373.9375}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFAm6id93gj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Parameters (papermill)\"\"\"\n",
        "\n",
        "# Recommend top k items\n",
        "TOP_K = 10\n",
        "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
        "MOVIELENS_DATA_SIZE = '100k'\n",
        "# Metrics to use for evaluation\n",
        "RANKING_METRICS = [\n",
        "    evaluator.ndcg_at_k.__name__,\n",
        "    evaluator.precision_at_k.__name__,\n",
        "]\n",
        "RATING_METRICS = [\n",
        "    evaluator.rmse.__name__,\n",
        "    evaluator.mae.__name__,\n",
        "]\n",
        "# Use session hook to evaluate model while training\n",
        "EVALUATE_WHILE_TRAINING = True\n",
        "# Item feature column name\n",
        "ITEM_FEAT_COL = 'genres'\n",
        "\n",
        "RANDOM_SEED = SEED  # Set seed for deterministic result\n",
        "\n",
        "# Train and test set pickle file paths. If provided, use them. Otherwise, download the MovieLens dataset.\n",
        "DATA_DIR = None\n",
        "TRAIN_PICKLE_PATH = None\n",
        "TEST_PICKLE_PATH = None\n",
        "EXPORT_DIR_BASE = './outputs/model'\n",
        "# Model checkpoints directory. If None, use temp-dir.\n",
        "MODEL_DIR = None\n",
        "\n",
        "#### Hyperparameters\n",
        "MODEL_TYPE = 'wide_deep'\n",
        "STEPS = 50000  # Number of batches to train\n",
        "BATCH_SIZE = 32\n",
        "# Wide (linear) model hyperparameters\n",
        "LINEAR_OPTIMIZER = 'adagrad'\n",
        "LINEAR_OPTIMIZER_LR = 0.0621  # Learning rate\n",
        "LINEAR_L1_REG = 0.0           # Regularization rate for FtrlOptimizer\n",
        "LINEAR_L2_REG = 0.0\n",
        "LINEAR_MOMENTUM = 0.0         # Momentum for MomentumOptimizer or RMSPropOptimizer\n",
        "# DNN model hyperparameters\n",
        "DNN_OPTIMIZER = 'adadelta'\n",
        "DNN_OPTIMIZER_LR = 0.1\n",
        "DNN_L1_REG = 0.0           # Regularization rate for FtrlOptimizer\n",
        "DNN_L2_REG = 0.0\n",
        "DNN_MOMENTUM = 0.0         # Momentum for MomentumOptimizer or RMSPropOptimizer\n",
        "# Layer dimensions. Defined as follows to make this notebook runnable from Hyperparameter tuning services like AzureML Hyperdrive\n",
        "DNN_HIDDEN_LAYER_1 = 0     # Set 0 to not use this layer\n",
        "DNN_HIDDEN_LAYER_2 = 64    # Set 0 to not use this layer\n",
        "DNN_HIDDEN_LAYER_3 = 128   # Set 0 to not use this layer\n",
        "DNN_HIDDEN_LAYER_4 = 512   # Note, at least one layer should have nodes.\n",
        "DNN_HIDDEN_UNITS = [h for h in [DNN_HIDDEN_LAYER_1, DNN_HIDDEN_LAYER_2, DNN_HIDDEN_LAYER_3, DNN_HIDDEN_LAYER_4] if h > 0]\n",
        "DNN_USER_DIM = 32          # User embedding feature dimension\n",
        "DNN_ITEM_DIM = 16          # Item embedding feature dimension\n",
        "DNN_DROPOUT = 0.8\n",
        "DNN_BATCH_NORM = 1         # 1 to use batch normalization, 0 if not."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RviVQsda4JLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MODEL_DIR is None:\n",
        "    TMP_DIR = TemporaryDirectory()\n",
        "    model_dir = TMP_DIR.name\n",
        "else:\n",
        "    if os.path.exists(MODEL_DIR) and os.listdir(MODEL_DIR):\n",
        "        raise ValueError(\n",
        "            \"Model exists in {}. Use different directory name or \"\n",
        "            \"remove the existing checkpoint files first\".format(MODEL_DIR)\n",
        "        )\n",
        "    TMP_DIR = None\n",
        "    model_dir = MODEL_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c058iqp44MAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "3cb67cc9-3623-400d-f7d7-ead6b73bccca"
      },
      "source": [
        "use_preset = (TRAIN_PICKLE_PATH is not None and TEST_PICKLE_PATH is not None)\n",
        "if not use_preset:\n",
        "    # The genres of each movie are returned as '|' separated string, e.g. \"Animation|Children's|Comedy\".\n",
        "    data = movielens.load_pandas_df(\n",
        "        size=MOVIELENS_DATA_SIZE,\n",
        "        header=[USER_COL, ITEM_COL, RATING_COL],\n",
        "        genres_col=ITEM_FEAT_COL\n",
        "    )\n",
        "    display(data.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.81k/4.81k [00:01<00:00, 3.39kKB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>rating</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>226</td>\n",
              "      <td>242</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>154</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>306</td>\n",
              "      <td>242</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userID  itemID  rating  genres\n",
              "0     196     242     3.0  Comedy\n",
              "1      63     242     3.0  Comedy\n",
              "2     226     242     5.0  Comedy\n",
              "3     154     242     3.0  Comedy\n",
              "4     306     242     5.0  Comedy"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnmJisdi4SGH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "eb76a258-033e-404f-e1b0-95a4164249ef"
      },
      "source": [
        "if not use_preset and ITEM_FEAT_COL is not None:\n",
        "    # Encode 'genres' into int array (multi-hot representation) to use as item features\n",
        "    genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
        "    data[ITEM_FEAT_COL] = genres_encoder.fit_transform(\n",
        "        data[ITEM_FEAT_COL].apply(lambda s: s.split(\"|\"))\n",
        "    ).tolist()\n",
        "    print(\"Genres:\", genres_encoder.classes_)\n",
        "    display(data.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Genres: ['Action' 'Adventure' 'Animation' \"Children's\" 'Comedy' 'Crime'\n",
            " 'Documentary' 'Drama' 'Fantasy' 'Film-Noir' 'Horror' 'Musical' 'Mystery'\n",
            " 'Romance' 'Sci-Fi' 'Thriller' 'War' 'Western' 'unknown']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>rating</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>226</td>\n",
              "      <td>242</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>154</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>306</td>\n",
              "      <td>242</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userID  itemID  rating                                             genres\n",
              "0     196     242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1      63     242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2     226     242     5.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3     154     242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4     306     242     5.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YhU16u45DCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "680f8ad7-17b5-4300-e417-542ae42bd235"
      },
      "source": [
        "if not use_preset:\n",
        "    train, test = python_random_split(data, ratio=0.8, seed=RANDOM_SEED)\n",
        "else:\n",
        "    train = pd.read_pickle(path=TRAIN_PICKLE_PATH if DATA_DIR is None else os.path.join(DATA_DIR, TRAIN_PICKLE_PATH))\n",
        "    test = pd.read_pickle(path=TEST_PICKLE_PATH if DATA_DIR is None else os.path.join(DATA_DIR, TEST_PICKLE_PATH))\n",
        "    data = pd.concat([train, test])\n",
        "\n",
        "print(\"{} train samples and {} test samples\".format(len(train), len(test)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80000 train samples and 20000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjYjpjKZ7oCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a28265d5-312c-4ae9-ca32-26d1629c2717"
      },
      "source": [
        "# Unique items in the dataset\n",
        "if ITEM_FEAT_COL is None:\n",
        "    items = data.drop_duplicates(ITEM_COL)[[ITEM_COL]].reset_index(drop=True)\n",
        "    item_feat_shape = None\n",
        "else:\n",
        "    items = data.drop_duplicates(ITEM_COL)[[ITEM_COL, ITEM_FEAT_COL]].reset_index(drop=True)\n",
        "    item_feat_shape = len(items[ITEM_FEAT_COL][0])\n",
        "# Unique users in the dataset\n",
        "users = data.drop_duplicates(USER_COL)[[USER_COL]].reset_index(drop=True)\n",
        "\n",
        "print(\"Total {} items and {} users in the dataset\".format(len(items), len(users)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 1682 items and 943 users in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jifdFfRM7s78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model checkpoint every n steps. We store the model 5 times.\n",
        "save_checkpoints_steps = max(1, STEPS // 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsmOSi4x7v-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "79ec2d25-c55c-485b-d752-cb6d85a02890"
      },
      "source": [
        "# Define wide (linear) and deep (dnn) features\n",
        "wide_columns, deep_columns = wide_deep.build_feature_columns(\n",
        "    users=users[USER_COL].values,\n",
        "    items=items[ITEM_COL].values,\n",
        "    user_col=USER_COL,\n",
        "    item_col=ITEM_COL,\n",
        "    item_feat_col=ITEM_FEAT_COL,\n",
        "    crossed_feat_dim=1000,\n",
        "    user_dim=DNN_USER_DIM,\n",
        "    item_dim=DNN_ITEM_DIM,\n",
        "    item_feat_shape=item_feat_shape,\n",
        "    model_type=MODEL_TYPE,\n",
        ")\n",
        "\n",
        "print(\"Wide feature specs:\")\n",
        "for c in wide_columns:\n",
        "    print(\"\\t\", str(c)[:100], \"...\")\n",
        "print(\"Deep feature specs:\")\n",
        "for c in deep_columns:\n",
        "    print(\"\\t\", str(c)[:100], \"...\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wide feature specs:\n",
            "\t VocabularyListCategoricalColumn(key='userID', vocabulary_list=(196, 63, 226, 154, 306, 296, 34, 271, ...\n",
            "\t VocabularyListCategoricalColumn(key='itemID', vocabulary_list=(242, 302, 377, 51, 346, 474, 265, 465 ...\n",
            "\t CrossedColumn(keys=(VocabularyListCategoricalColumn(key='userID', vocabulary_list=(196, 63, 226, 154 ...\n",
            "Deep feature specs:\n",
            "\t EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='userID', vocabulary_list=(19 ...\n",
            "\t EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='itemID', vocabulary_list=(24 ...\n",
            "\t NumericColumn(key='genres', shape=(19,), default_value=None, dtype=tf.float32, normalizer_fn=None) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VbhlLQu7zrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "944ac73e-e139-4cef-e4e1-530b5d262911"
      },
      "source": [
        "# Build a model based on the parameters\n",
        "model = wide_deep.build_model(\n",
        "    model_dir=model_dir,\n",
        "    wide_columns=wide_columns,\n",
        "    deep_columns=deep_columns,\n",
        "    linear_optimizer=tf_utils.build_optimizer(LINEAR_OPTIMIZER, LINEAR_OPTIMIZER_LR, **{\n",
        "        'l1_regularization_strength': LINEAR_L1_REG,\n",
        "        'l2_regularization_strength': LINEAR_L2_REG,\n",
        "        'momentum': LINEAR_MOMENTUM,\n",
        "    }),\n",
        "    dnn_optimizer=tf_utils.build_optimizer(DNN_OPTIMIZER, DNN_OPTIMIZER_LR, **{\n",
        "        'l1_regularization_strength': DNN_L1_REG,\n",
        "        'l2_regularization_strength': DNN_L2_REG,\n",
        "        'momentum': DNN_MOMENTUM,  \n",
        "    }),\n",
        "    dnn_hidden_units=DNN_HIDDEN_UNITS,\n",
        "    dnn_dropout=DNN_DROPOUT,\n",
        "    dnn_batch_norm=(DNN_BATCH_NORM==1),\n",
        "    log_every_n_iter=max(1, STEPS//10),  # log 10 times\n",
        "    save_checkpoints_steps=save_checkpoints_steps,\n",
        "    seed=RANDOM_SEED\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp07ph7akv', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3b71a00dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWYAxZ0w74yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = {\n",
        "    'col_user': USER_COL,\n",
        "    'col_item': ITEM_COL,\n",
        "    'col_rating': RATING_COL,\n",
        "    'col_prediction': PREDICT_COL,\n",
        "}\n",
        "\n",
        "# Prepare ranking evaluation set, i.e. get the cross join of all user-item pairs\n",
        "ranking_pool = user_item_pairs(\n",
        "    user_df=users,\n",
        "    item_df=items,\n",
        "    user_col=USER_COL,\n",
        "    item_col=ITEM_COL,\n",
        "    user_item_filter_df=train,  # Remove seen items\n",
        "    shuffle=True,\n",
        "    seed=RANDOM_SEED\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKG9YaZm8Bgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training hooks to track performance while training\n",
        "hooks = []\n",
        "if EVALUATE_WHILE_TRAINING:\n",
        "    evaluation_logger = tf_utils.MetricsLogger()\n",
        "    for metrics in (RANKING_METRICS, RATING_METRICS):\n",
        "        if len(metrics) > 0:\n",
        "            hooks.append(\n",
        "                tf_utils.evaluation_log_hook(\n",
        "                    model,\n",
        "                    logger=evaluation_logger,\n",
        "                    true_df=test,\n",
        "                    y_col=RATING_COL,\n",
        "                    eval_df=ranking_pool if metrics==RANKING_METRICS else test.drop(RATING_COL, axis=1),\n",
        "                    every_n_iter=save_checkpoints_steps,\n",
        "                    model_dir=model_dir,\n",
        "                    eval_fns=[evaluator.metrics[m] for m in metrics],\n",
        "                    **({**cols, 'k': TOP_K} if metrics==RANKING_METRICS else cols)\n",
        "                )\n",
        "            )\n",
        "\n",
        "# Define training input (sample feeding) function\n",
        "train_fn = tf_utils.pandas_input_fn(\n",
        "    df=train,\n",
        "    y_col=RATING_COL,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_epochs=None,  # We use steps=TRAIN_STEPS instead.\n",
        "    shuffle=True,\n",
        "    seed=RANDOM_SEED,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4ZMkUna8E1P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e04213fd-dd08-4be2-cd2f-b2bf5da0f29b"
      },
      "source": [
        "print(\n",
        "    \"Training steps = {}, Batch size = {} (num epochs = {})\"\n",
        "    .format(STEPS, BATCH_SIZE, (STEPS*BATCH_SIZE)//len(train))\n",
        ")\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "try:\n",
        "    model.train(\n",
        "        input_fn=train_fn,\n",
        "        hooks=hooks,\n",
        "        steps=STEPS\n",
        "    )\n",
        "except tf.train.NanLossDuringTrainingError:\n",
        "    import warnings\n",
        "    warnings.warn(\n",
        "        \"Training stopped with NanLossDuringTrainingError. \"\n",
        "        \"Try other optimizers, smaller batch size and/or smaller learning rate.\"\n",
        "    )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training steps = 50000, Batch size = 32 (num epochs = 20)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/feature_column/feature_column_v2.py:3079: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/feature_column/feature_column_v2.py:305: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:274: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:275: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp07ph7akv/model.ckpt.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:282: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:293: The name tf.logging.get_verbosity is deprecated. Please use tf.compat.v1.logging.get_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/recommenders/reco_utils/common/tf_utils.py:294: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
            "\n",
            "INFO:tensorflow:loss = 459.2428, step = 0\n",
            "INFO:tensorflow:global_step/sec: 30.0958\n",
            "INFO:tensorflow:loss = 41.640854, step = 5000 (90.445 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/tmp07ph7akv/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 53.9851\n",
            "INFO:tensorflow:loss = 42.332024, step = 10000 (168.815 sec)\n",
            "INFO:tensorflow:global_step/sec: 29.9818\n",
            "INFO:tensorflow:loss = 17.399158, step = 15000 (90.567 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 20000 into /tmp/tmp07ph7akv/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 55.9582\n",
            "INFO:tensorflow:loss = 28.749777, step = 20000 (163.526 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.5067\n",
            "INFO:tensorflow:loss = 19.12369, step = 25000 (89.725 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 30000 into /tmp/tmp07ph7akv/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 56.129\n",
            "INFO:tensorflow:loss = 29.03584, step = 30000 (163.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.5124\n",
            "INFO:tensorflow:loss = 15.404023, step = 35000 (89.807 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 40000 into /tmp/tmp07ph7akv/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 55.9736\n",
            "INFO:tensorflow:loss = 15.181356, step = 40000 (163.691 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6055\n",
            "INFO:tensorflow:loss = 26.707134, step = 45000 (89.004 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 50000 into /tmp/tmp07ph7akv/model.ckpt.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Loss for final step: 23.990646.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXeJ1TC98L49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "4c37647f-2ae6-4c76-bf33-98368664e91b"
      },
      "source": [
        "import scrapbook as sb\n",
        "if EVALUATE_WHILE_TRAINING:\n",
        "    logs = evaluation_logger.get_log()\n",
        "    for i, (m, v) in enumerate(logs.items(), 1):\n",
        "        pm.record(\"eval_{}\".format(m), v)\n",
        "        x = [save_checkpoints_steps*i for i in range(1, len(v)+1)]\n",
        "        plot.line_graph(\n",
        "            values=list(zip(v, x)),\n",
        "            labels=m,\n",
        "            x_name=\"steps\",\n",
        "            y_name=m,\n",
        "            subplot=(math.ceil(len(logs)/2), 2, i),\n",
        "        )"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-6ca5ff24866e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msave_checkpoints_steps\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         plot.line_graph(\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'papermill' has no attribute 'record'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Va8knkYav9s",
        "colab_type": "text"
      },
      "source": [
        "## Item rating prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ExUbPb903S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if len(RATING_METRICS) > 0:\n",
        "    predictions = list(model.predict(input_fn=tf_utils.pandas_input_fn(df=test)))\n",
        "    prediction_df = test.drop(RATING_COL, axis=1)\n",
        "    prediction_df[PREDICT_COL] = [p['predictions'][0] for p in predictions]\n",
        "    \n",
        "    rating_results = {}\n",
        "    for m in RATING_METRICS:\n",
        "        result = evaluator.metrics[m](test, prediction_df, **cols)\n",
        "        pm.record(m, result)\n",
        "        rating_results[m] = result\n",
        "    print(rating_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3eh4Bema4sk",
        "colab_type": "text"
      },
      "source": [
        "## Recommend k items\n",
        "For top-k recommendation evaluation, we use the ranking pool (all the user-item pairs) we prepared at the training step. The difference is we remove users' seen items from the pool in this step which is more natural to the movie recommendation scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uqFu65Ha1Gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if len(RANKING_METRICS) > 0:\n",
        "    predictions = list(model.predict(input_fn=tf_utils.pandas_input_fn(df=ranking_pool)))\n",
        "    prediction_df = ranking_pool.copy()\n",
        "    prediction_df[PREDICT_COL] = [p['predictions'][0] for p in predictions]\n",
        "\n",
        "    ranking_results = {}\n",
        "    for m in RANKING_METRICS:\n",
        "        result = evaluator.metrics[m](test, prediction_df, **{**cols, 'k': TOP_K})\n",
        "        pm.record(m, result)\n",
        "        ranking_results[m] = result\n",
        "    print(ranking_results)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}